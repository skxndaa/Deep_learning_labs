{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network mindset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/Tr.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"images\"][:]) \n",
    "    train_set_y_orig = np.array(train_dataset[\"labels\"][:]) \n",
    "\n",
    "    test_dataset = h5py.File('datasets/Te.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"images\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"labels\"][:]) \n",
    "\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    X_train = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T / 255.0\n",
    "    X_test = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T / 255.0\n",
    "\n",
    "    Y_train = np.eye(5)[train_set_y_orig.flatten()].T \n",
    "    Y_test = np.eye(5)[test_set_y_orig.flatten()].T \n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True)) \n",
    "    return exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(Y * np.log(AL + 1e-8)) / m \n",
    "    return np.squeeze(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    return Z\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "\n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(0, Z)\n",
    "    elif activation == 'softmax':\n",
    "        A = softmax(Z)\n",
    "\n",
    "    return A, Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, A_prev, W):\n",
    "    m = A_prev.shape[1]\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, Z, A_prev, W, activation):\n",
    "    if activation == 'relu':\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "    elif activation == 'softmax':\n",
    "        dZ = dA \n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, A_prev, W)\n",
    "\n",
    "    return dA_prev, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, learning_rate=0.0075, num_iterations=3000):\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        A = X\n",
    "        caches = []\n",
    "\n",
    "        for l in range(1, len(layer_dims) - 1):\n",
    "            A_prev = A\n",
    "            A, Z = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "            caches.append((A_prev, Z, parameters['W' + str(l)], parameters['b' + str(l)]))\n",
    "\n",
    "        AL, ZL = linear_activation_forward(A, parameters['W' + str(len(layer_dims) - 1)], \n",
    "                                           parameters['b' + str(len(layer_dims) - 1)], 'softmax')\n",
    "        caches.append((A, ZL, parameters['W' + str(len(layer_dims) - 1)], \n",
    "                       parameters['b' + str(len(layer_dims) - 1)]))\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "        costs.append(cost)\n",
    "\n",
    "        grads = {}\n",
    "        dA = AL - Y \n",
    "\n",
    "        for l in reversed(range(1, len(layer_dims))):\n",
    "            A_prev, Z, W, b = caches[l-1]\n",
    "            dA, dW, db = linear_activation_backward(dA, Z, A_prev, W, 'relu' if l != len(layer_dims) - 1 else 'softmax')\n",
    "            grads['dW' + str(l)] = dW\n",
    "            grads['db' + str(l)] = db\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.6094419485160683\n",
      "Cost after iteration 100: 1.6094390869035748\n",
      "Cost after iteration 200: 1.609437500178019\n",
      "Cost after iteration 300: 1.6094361924273595\n",
      "Cost after iteration 400: 1.6094351149720338\n",
      "Cost after iteration 500: 1.6094342434255346\n",
      "Cost after iteration 600: 1.6094333901317834\n",
      "Cost after iteration 700: 1.6094326130551977\n",
      "Cost after iteration 800: 1.6094319558735575\n",
      "Cost after iteration 900: 1.6094313543267271\n",
      "Train Accuracy: 28.80%\n",
      "Test Accuracy: 20.00%\n"
     ]
    }
   ],
   "source": [
    "def predict(X, parameters, layer_dims):\n",
    "    A = X\n",
    "    for l in range(1, len(layer_dims) - 1):\n",
    "        A, _ = linear_activation_forward(A, parameters['W' + str(l)], parameters['b' + str(l)], 'relu')\n",
    "\n",
    "    AL, _ = linear_activation_forward(A, parameters['W' + str(len(layer_dims) - 1)], \n",
    "                                      parameters['b' + str(len(layer_dims) - 1)], 'softmax')\n",
    "    predictions = np.argmax(AL, axis=0)\n",
    "    return predictions\n",
    "\n",
    "layer_dims = [X_train.shape[0], 64, 32, 16, 5] \n",
    "parameters = model(X_train, Y_train, layer_dims, learning_rate=0.01, num_iterations=1000)\n",
    "\n",
    "train_predictions = predict(X_train, parameters, layer_dims)\n",
    "test_predictions = predict(X_test, parameters, layer_dims)\n",
    "\n",
    "train_accuracy = np.mean(train_predictions == np.argmax(Y_train, axis=0)) * 100\n",
    "test_accuracy = np.mean(test_predictions == np.argmax(Y_test, axis=0)) * 100\n",
    "\n",
    "print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
